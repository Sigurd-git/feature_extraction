import torch
import torchaudio
import os
import hdf5storage
import numpy as np
from general_analysis_code.preprocess import align_time

def generate_whisper_features(
    device, whisper_model, wav_path, output_root, n_t, language="en", out_sr=100
):
    """
    Function to generate whisper features from an audio file.

    Parameters:
    device (torch.device): The device to run the model on (CPU or GPU).
    whisper_model: The pretrained whisper model, which is generated by:
        model = whisper.load_model("large")
    wav_path (str): The path to the audio file.
    output_root (str): The root directory where the output features will be saved.
    n_t (int): The number of time steps.
    out_sr (int): The sample rate of the output features.

    The function saves the generated features as .mat files and also creates a .txt file for each layer of features.

    The start time of output of layer n is: t_0_{n+1} = t_0_{n} + (conv_kernel_{n}-1) / (2*sr_{n})
    The sr of output of layer n is: sr_{n+1} = sr_{n} / conv_stride_{n}

    The downsampling is achieved through conv1d, so the time axis of features here is 1/2 of the original audio.
    """
    from whisper.audio import SAMPLE_RATE as WHISPER_SAMPLE_RATE

    wav_name = os.path.basename(wav_path)
    wav_name_no_ext = os.path.splitext(wav_name)[0]
    feature_class_out_dir = os.path.join(output_root, "features", "wav2vec2")
    meta_out_path = os.path.join(output_root, "feature_metadata", "wav2vec2.txt")
    if not os.path.exists(feature_class_out_dir):
        os.makedirs(feature_class_out_dir)
    if not os.path.exists(os.path.dirname(meta_out_path)):
        os.makedirs(os.path.dirname(meta_out_path))

    waveform, sample_rate = torchaudio.load(wav_path)
    t_num_new = int(np.round(waveform.shape[0] / WHISPER_SAMPLE_RATE * out_sr))
    if n_t is not None:
        t_new = np.arange(n_t) / out_sr
    else:
        t_new = np.arange(t_num_new) / out_sr
    conv_strides = [1, 2]
    if sample_rate != WHISPER_SAMPLE_RATE:
        waveform = torchaudio.functional.resample(
            waveform, sample_rate, WHISPER_SAMPLE_RATE
        )
    waveform = torch.nn.functional.pad(waveform, (0, 1280), "constant", 0)
    waveform = waveform.to(device).reshape(-1)

    with torch.inference_mode():
        results = whisper_model.transcribe(
            waveform, word_timestamps=True, language=language
        )
    segments = results["segments"]
    encoder_embeddings = [
        seg["encoder_embeddings"] for seg in segments
    ]  # batch, layer, time, feature
    decoder_embeddings = [
        seg["decoder_embeddings"] for seg in segments
    ]  # batch, layer, time, feature
    encoder_embeddings = np.concatenate(
        encoder_embeddings, axis=2
    )  # batch, layer, time, feature
    decoder_embeddings = np.concatenate(
        decoder_embeddings, axis=2
    )  # batch, layer, time, feature
    encoder_embeddings = encoder_embeddings[0]  # layer, time, feature
    decoder_embeddings = decoder_embeddings[0]  # layer, time, feature
    conv1 = results["conv1"]
    conv1 = np.concatenate(conv1, axis=1)  # batch, time, feature
    conv2 = results["conv2"]
    conv2 = np.concatenate(conv2, axis=1)  # batch, time, feature
    conv1 = conv1[0]  # time, feature
    conv2 = conv2[0]  # time, feature
    sr = WHISPER_SAMPLE_RATE / 160  # mel sr
    t_0 = 0
    for i, feats in enumerate([conv1, conv2]):
        # The start time of output of layer n is: t_0_{n+1} = t_0_{n} + (conv_kernel_{n}-1) / (2*sr_{n})
        # The sr of output of layer n is: sr_{n+1} = sr_{n} / conv_stride_{n}
        sr = sr / conv_strides[i]
        print(f"t_0_{i}={t_0}, sr_{i}={sr}")
        feature_variant_out_dir = os.path.join(feature_class_out_dir, f"layer{i}")
        if not os.path.exists(feature_variant_out_dir):
            os.makedirs(feature_variant_out_dir)
        ### align time to start from 0 and make the length to be n_t
        t_length = feats.shape[0]
        t_origin = np.arange(t_length) / sr + t_0

        feats = align_time(feats, t_origin, t_new, "t f")
        print(f"Feature {i}: {feats.shape}")

        # save each layer as mat
        out_mat_path = os.path.join(feature_variant_out_dir, f"{wav_name_no_ext}.mat")

        # save data as mat
        hdf5storage.savemat(out_mat_path, {"features": feats})
        # generate meta file for this layer
        with open(
            os.path.join(feature_variant_out_dir, f"{wav_name_no_ext}.txt"), "w"
        ) as f:
            f.write(f"""The shape of this conv layer is {feats.shape}.""")
    for j, feats in enumerate(encoder_embeddings):
        index = i + j + 1
        # The start time of output of layer n is: t_0_{n+1} = t_0_{n} + (conv_kernel_{n}-1) / (2*sr_{n})
        # The sr of output of layer n is: sr_{n+1} = sr_{n} / conv_stride_{n}
        print(f"t_0_{index}={t_0}, sr_{index}={sr}")
        feature_variant_out_dir = os.path.join(feature_class_out_dir, f"layer{index}")
        if not os.path.exists(feature_variant_out_dir):
            os.makedirs(feature_variant_out_dir)
        ### align time to start from 0 and make the length to be n_t
        t_length = feats.shape[0]
        t_origin = np.arange(t_length) / sr + t_0

        feats = align_time(feats, t_origin, t_new, "t f")
        print(f"Feature {index}: {feats.shape}")

        # save each layer as mat
        out_mat_path = os.path.join(feature_variant_out_dir, f"{wav_name_no_ext}.mat")

        # save data as mat
        hdf5storage.savemat(out_mat_path, {"features": feats})
        # generate meta file for this layer
        with open(
            os.path.join(feature_variant_out_dir, f"{wav_name_no_ext}.txt"), "w"
        ) as f:
            f.write(f"""The shape of this transformer layer is {feats.shape}.""")

    # generate meta file for this feature
    with open(meta_out_path, "w") as f:
        f.write(
            """whisper features. 
Each layer is saved separately as variant. 
Timestamps start from 0 ms, sr=100Hz. You can find out the shape of each stimulus in features/cochleagram/hubert/<layer>/<stimuli>.txt as (n_time,n_frequency). 
The timing (in seconds) of each time stamp can be computed like: timing=np.arange(n_time)/sr.
conv_stride = (1,2) and conv_kernel = (3,3), conv_padding=(1,1) for the convolusion layers.
There are 2 conv layers and 32 transformer layers.
"""
        )