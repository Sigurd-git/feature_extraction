import os
import hdf5storage
import numpy as np
import torch
import torchaudio
from general_analysis_code.preprocess import align_time
from utils.hook import register_activation_hooks, remove_hooks
from utils.pc import generate_pc


def generate_HUBERT_features(
    device, HUBERT_model, wav_path, output_root, n_t, out_sr=100
):
    """
    This function generates HUBERT features for a given audio file.

    Parameters:
    device (torch.device): The device on which to perform computations (CPU or GPU).
    HUBERT_model (nn.Module): The pre-trained HUBERT model, which is generated by:
        HUBERT_bundle = torchaudio.pipelines.HUBERT_BASE
        HUBERT_model = HUBERT_bundle.get_model().to(device)
    wav_path (str): The path to the .wav file for which to generate features.
    output_root (str): The root directory where the output feature files will be saved.
    n_t (int): The number of time steps.
    out_sr (int): The sample rate of the output features.

    The function saves the generated features as .mat files and also creates a .txt file for each layer of features.

    The start time of output of layer n is: t_0_{n+1} = t_0_{n} + (conv_kernel_{n}-1) / (2*sr_{n})
    The sr of output of layer n is: sr_{n+1} = sr_{n} / conv_stride_{n}

    How I derive the time of first point: The downsampling is achieved through conv1d, so the time axis of features here is 1/320 of the original audio. conv_stride = (5, 2, 2, 2, 2, 2, 2) and conv_kernel = (10, 3, 3, 3 ,3 ,2 ,2). Therefore, the time points of the original audio are:
        conv1: (0+9)/2, (5+14)/2, ... = 4.5, 9.5, ... stride=5, kernel=10
        conv2: (4.5+14.5)/2, (14.5+24.5)/2, ... = 9.5, 19.5, ... stride=2, kernel=3
        conv3: (9.5+29.5)/2, (19.5+39.5)/2, ... = 19.5, 39.5, ... stride=2, kernel=3
        conv4: (19.5+59.5)/2, (39.5+79.5)/2, ... = 39.5, 79.5, ... stride=2, kernel=3
        conv5: (39.5+119.5)/2, (79.5+159.5)/2, ... = 79.5, 159.5, ... stride=2, kernel=3
        conv6: (79.5+159.5)/2, (239.5+319.5)/2, ... = 119.5, 279.5, ... stride=2, kernel=2
        conv7: (119.5+279.5)/2, (439.5+599.5)/2, ... = 199.5, 519.5, ... stride=2, kernel=2
    """

    wav_name = os.path.basename(wav_path)
    wav_name_no_ext = os.path.splitext(wav_name)[0]
    feature_class_out_dir = os.path.join(output_root, "features", "hubert")
    meta_out_path = os.path.join(output_root, "feature_metadata", "hubert.txt")
    if not os.path.exists(feature_class_out_dir):
        os.makedirs(feature_class_out_dir)
    if not os.path.exists(os.path.dirname(meta_out_path)):
        os.makedirs(os.path.dirname(meta_out_path))
    waveform, sample_rate = torchaudio.load(wav_path)
    t_num_new = int(np.round(waveform.shape[1] / sample_rate * out_sr))
    if n_t is not None:
        t_new = np.arange(n_t) / out_sr
    else:
        t_new = np.arange(t_num_new) / out_sr
    waveform = waveform.to(device)
    HUBERT_bundle = torchaudio.pipelines.HUBERT_BASE  #    HUBERT_bundle (HUBERTBundle): The HUBERT bundle containing the sample rate and other information.
    conv_kernels = [i[1] for i in HUBERT_bundle._params["extractor_conv_layer_config"]]
    conv_strides = [i[2] for i in HUBERT_bundle._params["extractor_conv_layer_config"]]

    if sample_rate != HUBERT_bundle.sample_rate:
        waveform = torchaudio.functional.resample(
            waveform, sample_rate, HUBERT_bundle.sample_rate
        )

    # pad 0.5s before and pad 0.5s after
    pad_number = int(HUBERT_bundle.sample_rate / 2)
    waveform = torch.nn.functional.pad(
        waveform, (pad_number, pad_number), "constant", 0
    )
    hooks, activations = register_activation_hooks(HUBERT_model)
    with torch.inference_mode():
        features, _ = HUBERT_model.extract_features(waveform)
    remove_hooks(hooks)
    sr = HUBERT_bundle.sample_rate
    t_0 = -0.5

    for i, feats in enumerate(activations):
        # The start time of output of layer n is: t_0_{n+1} = t_0_{n} + (conv_kernel_{n}-1) / (2*sr_{n})
        # The sr of output of layer n is: sr_{n+1} = sr_{n} / conv_stride_{n}
        t_0 = t_0 + (conv_kernels[i] - 1) / (2 * sr)
        sr = sr / conv_strides[i]
        print(f"t_0_{i}={t_0}, sr_{i}={sr}")
        feature_variant_out_dir = os.path.join(feature_class_out_dir, f"layer{i}")
        if not os.path.exists(feature_variant_out_dir):
            os.makedirs(feature_variant_out_dir)
        feats = feats[0][0].squeeze(0)
        feats = feats.cpu().numpy().transpose()
        ### align time to start from 0 and make the length to be n_t
        t_length = feats.shape[0]
        t_origin = np.arange(t_length) / sr + t_0

        feats = align_time(feats, t_origin, t_new, "t f")
        print(f"Feature {i}: {feats.shape}")

        # save each layer as mat
        out_mat_path = os.path.join(feature_variant_out_dir, f"{wav_name_no_ext}.mat")

        # save data as mat
        hdf5storage.savemat(out_mat_path, {"features": feats})
        # generate meta file for this layer
        with open(
            os.path.join(feature_variant_out_dir, f"{wav_name_no_ext}.txt"), "w"
        ) as f:
            f.write(f"""The shape of this conv layer is {feats.shape}.""")

    for j, feats in enumerate(features):
        index = i + j + 1

        feature_variant_out_dir = os.path.join(feature_class_out_dir, f"layer{index}")
        if not os.path.exists(feature_variant_out_dir):
            os.makedirs(feature_variant_out_dir)
        feats = feats.squeeze(0)
        feats = feats.cpu().numpy()
        ### align time to start from 0 and make the length to be n_t
        t_length = feats.shape[0]
        t_origin = np.arange(t_length) / sr + t_0

        feats = align_time(feats, t_origin, t_new, "t f")
        print(f"Feature {index}: {feats.shape}")
        # save each layer as mat
        out_mat_path = os.path.join(feature_variant_out_dir, f"{wav_name_no_ext}.mat")

        # save data as mat
        hdf5storage.savemat(out_mat_path, {"features": feats})
        # generate meta file for this layer
        with open(
            os.path.join(feature_variant_out_dir, f"{wav_name_no_ext}.txt"), "w"
        ) as f:
            f.write(f"""The shape of this transformer layer is {feats.shape}.""")

    # generate meta file for this feature
    with open(meta_out_path, "w") as f:
        f.write(
            """HuBert features.
Each layer is saved separately as variant. 
Timestamps start from 0 ms, sr=100Hz. You can find out the shape of each stimulus in features/hubert/<layer>/<stimuli>.txt as (n_time,n_feature). 
The timing (in seconds) of each time stamp can be computed like: timing=np.arange(n_time)/sr.
conv_stride = (5, 2, 2, 2, 2, 2, 2) and conv_kernel = (10, 3, 3, 3 ,3 ,2 ,2), no padding for the convolusion layers.
There are 7 conv layers and 12 transformer layers.

"""
        )


def hubert(device, stim_names, output_root, wav_dir, sr=100, pc=100, **kwargs):
    compile_torch = kwargs.get("compile_torch", True)
    # TODO: Use large finetuned model
    HUBERT_bundle = torchaudio.pipelines.HUBERT_BASE
    HUBERT_model = HUBERT_bundle.get_model().to(device)
    if compile_torch:
        HUBERT_model = torch.compile(HUBERT_model)
    for stim_index, stim_name in enumerate(stim_names):
        wav_path = os.path.join(wav_dir, f"{stim_name}.wav")
        generate_HUBERT_features(
            device, HUBERT_model, wav_path, output_root, n_t=None, out_sr=sr
        )

    # compute PC of hubert features
    feature_name = "hubert"
    layer_num = 19
    for layer in range(layer_num):
        wav_features = []
        for stim_index, stim_name in enumerate(stim_names):
            wav_path = os.path.join(wav_dir, f"{stim_name}.wav")
            feature_path = (
                f"{output_root}/features/{feature_name}/layer{layer}/{stim_name}.mat"
            )
            feature = hdf5storage.loadmat(feature_path)["features"]
            wav_features.append(feature)
        pca_pipeline = generate_pc(
            wav_features,
            pc,
            output_root,
            feature_name,
            stim_names,
            demean=True,
            std=False,
            variant=f"layer{layer}",
        )
        generate_pc(
            wav_features,
            pc,
            output_root,
            feature_name,
            stim_names,
            demean=True,
            std=False,
            variant=f"layer{layer}",
            pca_pipeline=pca_pipeline,
        )
