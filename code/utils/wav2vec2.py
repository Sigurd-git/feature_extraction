import os
import hdf5storage
import numpy as np
import torch
import torchaudio
from general_analysis_code.preprocess import align_time
from hook import register_activation_hooks, remove_hooks
def generate_wav2vec2_features(
    device, wav2vec2_model, wav_path, output_root, n_t, out_sr=100
):
    """
    Function to generate wav2vec2 features from an audio file.

    Parameters:
    device (torch.device): The device to run the model on (CPU or GPU).
    wav2vec2_model (Wav2Vec2Model): The pretrained Wav2Vec2 model, which is generated by:
        wav2vec2_bundle = torchaudio.pipelines.WAV2VEC2_BASE
        wav2vec2_model = wav2vec2_bundle.get_model().to(device)
    wav_path (str): The path to the audio file.
    output_root (str): The root directory where the output features will be saved.
    n_t (int): The number of time steps.
    out_sr (int): The sample rate of the output features.

    The function saves the generated features as .mat files and also creates a .txt file for each layer of features.

    The start time of output of layer n is: t_0_{n+1} = t_0_{n} + (conv_kernel_{n}-1) / (2*sr_{n})
    The sr of output of layer n is: sr_{n+1} = sr_{n} / conv_stride_{n}

    How I derive the time of first point: The downsampling is achieved through conv1d, so the time axis of features here is 1/320 of the original audio. conv_stride = (5, 2, 2, 2, 2, 2, 2) and conv_kernel = (10, 3, 3, 3 ,3 ,2 ,2). Therefore, the time points of the original audio are:
        conv1: (0+9)/2, (5+14)/2, ... = 4.5, 9.5, ... stride=5, kernel=10
        conv2: (4.5+14.5)/2, (14.5+24.5)/2, ... = 9.5, 19.5, ... stride=2, kernel=3
        conv3: (9.5+29.5)/2, (19.5+39.5)/2, ... = 19.5, 39.5, ... stride=2, kernel=3
        conv4: (19.5+59.5)/2, (39.5+79.5)/2, ... = 39.5, 79.5, ... stride=2, kernel=3
        conv5: (39.5+119.5)/2, (79.5+159.5)/2, ... = 79.5, 159.5, ... stride=2, kernel=3
        conv6: (79.5+159.5)/2, (239.5+319.5)/2, ... = 119.5, 279.5, ... stride=2, kernel=2
        conv7: (119.5+279.5)/2, (439.5+599.5)/2, ... = 199.5, 519.5, ... stride=2, kernel=2
    """
    wav_name = os.path.basename(wav_path)
    wav_name_no_ext = os.path.splitext(wav_name)[0]
    feature_class_out_dir = os.path.join(output_root, "features", "wav2vec2")
    meta_out_path = os.path.join(output_root, "feature_metadata", "wav2vec2.txt")
    if not os.path.exists(feature_class_out_dir):
        os.makedirs(feature_class_out_dir)
    if not os.path.exists(os.path.dirname(meta_out_path)):
        os.makedirs(os.path.dirname(meta_out_path))

    waveform, sample_rate = torchaudio.load(wav_path)
    t_num_new = int(np.round(waveform.shape[1] / sample_rate * out_sr))
    if n_t is not None:
        t_new = np.arange(n_t) / out_sr
    else:
        t_new = np.arange(t_num_new) / out_sr
    waveform = waveform.to(device)

    wav2vec2_bundle = torchaudio.pipelines.WAV2VEC2_BASE  # wav2vec2_bundle (Wav2Vec2Bundle): The bundle containing the Wav2Vec2 model and its configuration.
    conv_kernels = [
        i[1] for i in wav2vec2_bundle._params["extractor_conv_layer_config"]
    ]
    conv_strides = [
        i[2] for i in wav2vec2_bundle._params["extractor_conv_layer_config"]
    ]
    if sample_rate != wav2vec2_bundle.sample_rate:
        waveform = torchaudio.functional.resample(
            waveform, sample_rate, wav2vec2_bundle.sample_rate
        )
    # pad 640 before and pad 640 after
    waveform = torch.nn.functional.pad(waveform, (640, 640), "constant", 0)
    hooks, activations = register_activation_hooks(wav2vec2_model)
    with torch.inference_mode():
        features, _ = wav2vec2_model.extract_features(waveform)
    remove_hooks(hooks)
    sr = wav2vec2_bundle.sample_rate
    t_0 = -640 / sr

    for i, feats in enumerate(activations):
        # The start time of output of layer n is: t_0_{n+1} = t_0_{n} + (conv_kernel_{n}-1) / (2*sr_{n})
        # The sr of output of layer n is: sr_{n+1} = sr_{n} / conv_stride_{n}
        t_0 = t_0 + (conv_kernels[i] - 1) / (2 * sr)
        sr = sr / conv_strides[i]
        print(f"t_0_{i}={t_0}, sr_{i}={sr}")
        feature_variant_out_dir = os.path.join(feature_class_out_dir, f"layer{i}")
        if not os.path.exists(feature_variant_out_dir):
            os.makedirs(feature_variant_out_dir)
        feats = feats[0][0].squeeze(0)
        feats = feats.cpu().numpy().transpose()
        ### align time to start from 0 and make the length to be n_t
        t_length = feats.shape[0]
        t_origin = np.arange(t_length) / sr + t_0

        feats = align_time(feats, t_origin, t_new, "t f")
        print(f"Feature {i}: {feats.shape}")

        # save each layer as mat
        out_mat_path = os.path.join(feature_variant_out_dir, f"{wav_name_no_ext}.mat")

        # save data as mat
        hdf5storage.savemat(out_mat_path, {"features": feats})
        # generate meta file for this layer
        with open(
            os.path.join(feature_variant_out_dir, f"{wav_name_no_ext}.txt"), "w"
        ) as f:
            f.write(f"""The shape of this conv layer is {feats.shape}.""")

    for j, feats in enumerate(features):
        index = i + j + 1

        feature_variant_out_dir = os.path.join(feature_class_out_dir, f"layer{index}")
        if not os.path.exists(feature_variant_out_dir):
            os.makedirs(feature_variant_out_dir)
        feats = feats.squeeze(0)
        feats = feats.cpu().numpy()

        ### align time to start from 0 and make the length to be n_t
        t_length = feats.shape[0]
        t_origin = np.arange(t_length) / sr + t_0

        feats = align_time(feats, t_origin, t_new, "t f")
        print(f"Feature {i}: {feats.shape}")

        # save each layer as mat
        out_mat_path = os.path.join(feature_variant_out_dir, f"{wav_name_no_ext}.mat")
        # save data as mat
        hdf5storage.savemat(out_mat_path, {"features": feats})
        # generate meta file for this layer
        with open(
            os.path.join(feature_variant_out_dir, f"{wav_name_no_ext}.txt"), "w"
        ) as f:
            f.write(f"""The shape of this rtransformer layer is {feats.shape}.""")

    # generate meta file for this feature
    with open(meta_out_path, "w") as f:
        f.write(
            """wav2vec2 features. 
Each layer is saved separately as variant. 
Timestamps start from 0 ms, sr=100Hz. You can find out the shape of each stimulus in features/cochleagram/hubert/<layer>/<stimuli>.txt as (n_time,n_frequency). 
The timing (in seconds) of each time stamp can be computed like: timing=np.arange(n_time)/sr.
conv_stride = (5, 2, 2, 2, 2, 2, 2) and conv_kernel = (10, 3, 3, 3 ,3 ,2 ,2), no padding for the convolusion layers.
There are 7 conv layers and 12 transformer layers.
"""
        )
